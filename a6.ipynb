{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4NcudHpmAUz1"
      },
      "source": [
        "# Programming a neural network layer\n",
        "\n",
        "[Keras](https://keras.io) is a high-level deep-learning framework building on top of [TensorFlow](https://www.tensorflow.org). These frameworks follow the _symbol-to-symbol derivatives_ approach, i.e. automatically derive a computational graph to calculate derivatives. You just need to declare your inputs as TensorFlow variables and use TensorFlow operations on them to compute the forward pass.  \n",
        "\n",
        "## Task 6.1\n",
        "\n",
        "Work through the [Keras tutorial on custom layers](https://keras.io/guides/making_new_layers_and_models_via_subclassing) to learn how to create your own neural network layer.  \n",
        "Create a custom Keras layer that computes Gaussian basis functions, i.e. a layer that maps an input vector $\\mathbf x \\in \\mathbb R^n$ onto an output vector $\\mathbf y = f(\\mathbf x) \\in \\mathbb R^m$ as follows:\n",
        "\\begin{align}\n",
        "  f: \\mathbf x \\in \\mathbb R^n \\mapsto \\left[w_i \\exp\\left(-\\frac{\\|\\mathbf x - \\boldsymbol\\mu_i\\|^2}{\\sigma_i^2}\\right)\\right]_{i=1..m} \\in \\mathbb R^m\n",
        "\\end{align}\n",
        "\n",
        "Instead of projecting an input $\\mathbf x$ onto a weight vector $\\mathbf w$ as the standard neuron function $f(\\mathbf x) = \\sigma(\\mathbf w \\cdot \\mathbf x + b)$ does, the Gaussian basis function becomes active (with weight $w_i$) for all inputs $\\mathbf x$ close to a prototype $\\boldsymbol \\mu_i$. This activation quickly decays with increasing distance of $\\mathbf x$ to $\\boldsymbol \\mu_i$. The parameter $\\sigma_i$ controls the width of the Gaussian, i.e. the size of the active region.\n",
        "\n",
        "For efficient tensor-based operations you need to correctly _broadcast_ the tensors for the difference operation: TensorFlow will pass an input matrix of shape `(batch size, input dim)` for $\\mathbf X$, while you will have a matrix of centers $\\boldsymbol \\mu$ of shape `(input dim, #units)`. To correctly [broadcast](https://numpy.org/doc/stable/user/basics.broadcasting.html) them together, you will need Keras' [`expand_dims()`](https://www.tensorflow.org/api_docs/python/tf/keras/backend/expand_dims) function to extend $\\mathbf X$'s shape to `(batch size, input dim, 1)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 5) (5, 2) (3, 5, 2)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class RBFLayer(keras.layers.Layer):\n",
        "    def __init__(self, output_dim, initializer=None, **kwargs):\n",
        "        super(RBFLayer, self).__init__(**kwargs)\n",
        "        self.output_dim = output_dim\n",
        "        self.initializer = initializer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.centers = self.add_weight(\n",
        "            name='centers',\n",
        "            shape=(self.output_dim, input_shape[-1]),\n",
        "            initializer=self.initializer,\n",
        "            trainable=True)\n",
        "        self.sigmas = self.add_weight(\n",
        "            name='sigmas',\n",
        "            shape=(self.output_dim,),\n",
        "            initializer='ones',\n",
        "            trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = K.expand_dims(inputs, 1)\n",
        "        diff = inputs - self.centers\n",
        "        norm = K.sum(K.square(diff), axis=-1)\n",
        "        norm = K.transpose(norm)\n",
        "        return K.exp(-norm / (2 * K.square(self.sigmas)))\n",
        "\n",
        "# Example usage:\n",
        "X = tf.ones((3, 5))  # input tensor X with batch dimension 3 and data dim N=5\n",
        "mu = tf.ones((5, 2))  # tensor mu with data dim N=5 and 2 units\n",
        "diffs = K.expand_dims(X, axis=-1) - mu  # diffs tensor: 3 x 5 x 2\n",
        "print(X.shape, mu.shape, diffs.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9hWz4ulrAUz4"
      },
      "source": [
        "## Task 6.2\n",
        "\n",
        "Compare the performance of such a Gaussian basis function layer with that of a standard [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer on the MNIST dataset.  \n",
        "Hint: Utilize existing tutorials on setting up your first MNIST MLP with Keras, e.g. https://www.tensorflow.org/guide/keras/train_and_evaluate.\n",
        "\n",
        "To achieve decent performance, you want to:\n",
        "- Initialize the centers $\\boldsymbol \\mu_i$ from random data samples $\\mathbf x$ (create a custom [initializer](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/Initializer))\n",
        "- Initialize $\\sigma_i$ to the typical in-class distance between data points.  \n",
        "  Use [`scipy.spatial.distance_matrix`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance_matrix.html) to compute this statistics on a random selection of your input data.  \n",
        "  (Doing it on the full dataset will probably exhaust your memory.)\n",
        "- Initialize $w_i = 1$\n",
        "\n",
        "Questions:\n",
        "- How many parameters each of those networks have?\n",
        "- Which network trains faster / easier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "750/750 [==============================] - 1s 1ms/step - loss: 0.3471 - sparse_categorical_accuracy: 0.9019 - val_loss: 0.1983 - val_sparse_categorical_accuracy: 0.9410\n",
            "Epoch 2/5\n",
            "750/750 [==============================] - 1s 921us/step - loss: 0.1674 - sparse_categorical_accuracy: 0.9507 - val_loss: 0.1483 - val_sparse_categorical_accuracy: 0.9561\n",
            "Epoch 3/5\n",
            "750/750 [==============================] - 1s 888us/step - loss: 0.1244 - sparse_categorical_accuracy: 0.9628 - val_loss: 0.1365 - val_sparse_categorical_accuracy: 0.9605\n",
            "Epoch 4/5\n",
            "750/750 [==============================] - 1s 876us/step - loss: 0.0980 - sparse_categorical_accuracy: 0.9702 - val_loss: 0.1156 - val_sparse_categorical_accuracy: 0.9659\n",
            "Epoch 5/5\n",
            "750/750 [==============================] - 1s 868us/step - loss: 0.0823 - sparse_categorical_accuracy: 0.9754 - val_loss: 0.1078 - val_sparse_categorical_accuracy: 0.9688\n",
            "79/79 [==============================] - 0s 581us/step - loss: 0.0968 - sparse_categorical_accuracy: 0.9707\n",
            "test loss, test acc: [0.09682253748178482, 0.9707000255584717]\n",
            "313/313 [==============================] - 0s 387us/step\n",
            "Accuracy score:0.9707\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.spatial import distance_matrix\n",
        "import numpy as np\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocessing the data\n",
        "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
        "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
        "\n",
        "y_train = y_train.astype(\"float32\")\n",
        "y_test = y_test.astype(\"float32\")\n",
        "\n",
        "# Get random set of 1000 mnist images\n",
        "randomset = x_train[np.random.choice(x_train.shape[0], 1000, replace=False)]\n",
        "distances = distance_matrix(randomset, randomset)\n",
        "\n",
        "# Initialize to typical in-class distances\n",
        "\n",
        "sigma = np.mean(distances)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class MuInitializer:\n",
        "    def __init__(self, randomset):\n",
        "        self.randomset = randomset\n",
        "\n",
        "    def __call__(self, shape, dtype=None):\n",
        "        low = -1.0 / np.sqrt(shape[1])\n",
        "        high = 1.0 / np.sqrt(shape[1])\n",
        "        return np.random.uniform(low=low, high=high, size=shape).astype(tf.keras.backend.floatx())\n",
        "\n",
        "# The Dense Layer model\n",
        "\n",
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\", kernel_initializer=\"glorot_uniform\")(inputs)\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\", kernel_initializer=\"glorot_uniform\")(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.2)\n",
        "\n",
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "\n",
        "print(\"test loss, test acc:\", results)\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "predicted_labels = tf.argmax(predictions, axis=1)\n",
        "accuracy = accuracy_score(y_test, predicted_labels)\n",
        "\n",
        "print(f\"Accuracy score:{accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The Gaussian RBF Layer model\n",
        "\n",
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = RBFLayer(64, initializer=MuInitializer(randomset))(inputs)\n",
        "x = RBFLayer(64, initializer=MuInitializer(randomset))(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.2)\n",
        "\n",
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "\n",
        "print(\"test loss, test acc:\", results)\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "predicted_labels = tf.argmax(predictions, axis=1)\n",
        "accuracy = accuracy_score(y_test, predicted_labels)\n",
        "\n",
        "print(f\"Accuracy score:{accuracy}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I can't run this code since the code from the first tasks seems to not run correctly. \n",
        "\n",
        "- How many parameters each of those networks have?\n",
        "\n",
        "For the Dense Layer Model:\n",
        "\n",
        "Layers = 785 * 64 = 50240 + 65 * 64 = 4160 + 650 = 55050\n",
        "\n",
        "For the gaussian layer:\n",
        "\n",
        "This depends on the dimension of the input and output. In this case it has 64*10 + 10 = 650 parameters.\n",
        "\n",
        "- Which network trains faster / easier?\n",
        "\n",
        "The Gaussian is slower than the Dense Layer model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
