{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4NcudHpmAUz1"
      },
      "source": [
        "# Programming a neural network layer\n",
        "\n",
        "[Keras](https://keras.io) is a high-level deep-learning framework building on top of [TensorFlow](https://www.tensorflow.org). These frameworks follow the _symbol-to-symbol derivatives_ approach, i.e. automatically derive a computational graph to calculate derivatives. You just need to declare your inputs as TensorFlow variables and use TensorFlow operations on them to compute the forward pass.  \n",
        "\n",
        "## Task 6.1\n",
        "\n",
        "Work through the [Keras tutorial on custom layers](https://keras.io/guides/making_new_layers_and_models_via_subclassing) to learn how to create your own neural network layer.  \n",
        "Create a custom Keras layer that computes Gaussian basis functions, i.e. a layer that maps an input vector $\\mathbf x \\in \\mathbb R^n$ onto an output vector $\\mathbf y = f(\\mathbf x) \\in \\mathbb R^m$ as follows:\n",
        "\\begin{align}\n",
        "  f: \\mathbf x \\in \\mathbb R^n \\mapsto \\left[w_i \\exp\\left(-\\frac{\\|\\mathbf x - \\boldsymbol\\mu_i\\|^2}{\\sigma_i^2}\\right)\\right]_{i=1..m} \\in \\mathbb R^m\n",
        "\\end{align}\n",
        "\n",
        "Instead of projecting an input $\\mathbf x$ onto a weight vector $\\mathbf w$ as the standard neuron function $f(\\mathbf x) = \\sigma(\\mathbf w \\cdot \\mathbf x + b)$ does, the Gaussian basis function becomes active (with weight $w_i$) for all inputs $\\mathbf x$ close to a prototype $\\boldsymbol \\mu_i$. This activation quickly decays with increasing distance of $\\mathbf x$ to $\\boldsymbol \\mu_i$. The parameter $\\sigma_i$ controls the width of the Gaussian, i.e. the size of the active region.\n",
        "\n",
        "For efficient tensor-based operations you need to correctly _broadcast_ the tensors for the difference operation: TensorFlow will pass an input matrix of shape `(batch size, input dim)` for $\\mathbf X$, while you will have a matrix of centers $\\boldsymbol \\mu$ of shape `(input dim, #units)`. To correctly [broadcast](https://numpy.org/doc/stable/user/basics.broadcasting.html) them together, you will need Keras' [`expand_dims()`](https://www.tensorflow.org/api_docs/python/tf/keras/backend/expand_dims) function to extend $\\mathbf X$'s shape to `(batch size, input dim, 1)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 5) (5, 2) (3, 5, 2)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class RBFLayer(keras.layers.Layer):\n",
        "    def __init__(self, output_dim, initializer=None, **kwargs):\n",
        "        super(RBFLayer, self).__init__(**kwargs)\n",
        "        self.output_dim = output_dim\n",
        "        self.initializer = initializer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.centers = self.add_weight(\n",
        "            name='centers',\n",
        "            shape=(self.output_dim, input_shape[1]),\n",
        "            initializer=self.initializer,\n",
        "            trainable=True)\n",
        "        self.sigmas = self.add_weight(\n",
        "            name='sigmas',\n",
        "            shape=(self.output_dim,),\n",
        "            initializer='ones',\n",
        "            trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = K.expand_dims(inputs, 1)\n",
        "        diff = inputs - self.centers\n",
        "        norm = K.sum(K.square(diff), axis=-1)\n",
        "        norm = K.transpose(norm)\n",
        "        return K.exp(-norm / (2 * K.square(self.sigmas)))\n",
        "\n",
        "# Example usage:\n",
        "X = tf.ones((3, 5))  # input tensor X with batch dimension 3 and data dim N=5\n",
        "mu = tf.ones((5, 2))  # tensor mu with data dim N=5 and 2 units\n",
        "diffs = K.expand_dims(X, axis=-1) - mu  # diffs tensor: 3 x 5 x 2\n",
        "print(X.shape, mu.shape, diffs.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9hWz4ulrAUz4"
      },
      "source": [
        "## Task 6.2\n",
        "\n",
        "Compare the performance of such a Gaussian basis function layer with that of a standard [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer on the MNIST dataset.  \n",
        "Hint: Utilize existing tutorials on setting up your first MNIST MLP with Keras, e.g. https://www.tensorflow.org/guide/keras/train_and_evaluate.\n",
        "\n",
        "To achieve decent performance, you want to:\n",
        "- Initialize the centers $\\boldsymbol \\mu_i$ from random data samples $\\mathbf x$ (create a custom [initializer](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/Initializer))\n",
        "- Initialize $\\sigma_i$ to the typical in-class distance between data points.  \n",
        "  Use [`scipy.spatial.distance_matrix`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance_matrix.html) to compute this statistics on a random selection of your input data.  \n",
        "  (Doing it on the full dataset will probably exhaust your memory.)\n",
        "- Initialize $w_i = 1$\n",
        "\n",
        "Questions:\n",
        "- How many parameters each of those networks have?\n",
        "- Which network trains faster / easier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "750/750 [==============================] - 1s 1ms/step - loss: 0.3599 - sparse_categorical_accuracy: 0.8975 - val_loss: 0.1889 - val_sparse_categorical_accuracy: 0.9473\n",
            "Epoch 2/5\n",
            "750/750 [==============================] - 1s 906us/step - loss: 0.1735 - sparse_categorical_accuracy: 0.9485 - val_loss: 0.1578 - val_sparse_categorical_accuracy: 0.9521\n",
            "Epoch 3/5\n",
            "750/750 [==============================] - 1s 860us/step - loss: 0.1247 - sparse_categorical_accuracy: 0.9625 - val_loss: 0.1313 - val_sparse_categorical_accuracy: 0.9607\n",
            "Epoch 4/5\n",
            "750/750 [==============================] - 1s 884us/step - loss: 0.0984 - sparse_categorical_accuracy: 0.9705 - val_loss: 0.1104 - val_sparse_categorical_accuracy: 0.9670\n",
            "Epoch 5/5\n",
            "750/750 [==============================] - 1s 849us/step - loss: 0.0816 - sparse_categorical_accuracy: 0.9752 - val_loss: 0.1105 - val_sparse_categorical_accuracy: 0.9662\n",
            "79/79 [==============================] - 0s 528us/step - loss: 0.1022 - sparse_categorical_accuracy: 0.9700\n",
            "test loss, test acc: [0.10224821418523788, 0.9700000286102295]\n",
            "313/313 [==============================] - 0s 363us/step\n",
            "Accuracy score:0.97\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.spatial import distance_matrix\n",
        "import numpy as np\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocessing the data\n",
        "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
        "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
        "\n",
        "y_train = y_train.astype(\"float32\")\n",
        "y_test = y_test.astype(\"float32\")\n",
        "\n",
        "# Get random set of 1000 mnist images\n",
        "randomset = x_train[np.random.choice(x_train.shape[0], 1000, replace=False)]\n",
        "distances = distance_matrix(randomset, randomset)\n",
        "\n",
        "# Initialize to typical in-class distances\n",
        "\n",
        "sigma = np.mean(distances)\n",
        "\n",
        "class MuInitializer(keras.initializers.Initializer):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __call__(self, shape, dtype=None):\n",
        "        centers = self.data[np.random.choice(len(self.data), size=shape[0])]\n",
        "        return tf.convert_to_tensor(centers, dtype=dtype)\n",
        "\n",
        "# The Dense Layer model\n",
        "\n",
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\", kernel_initializer=\"glorot_uniform\")(inputs)\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\", kernel_initializer=\"glorot_uniform\")(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.2)\n",
        "\n",
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "\n",
        "print(\"test loss, test acc:\", results)\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "predicted_labels = tf.argmax(predictions, axis=1)\n",
        "accuracy = accuracy_score(y_test, predicted_labels)\n",
        "\n",
        "print(f\"Accuracy score:{accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "In this `tf.Variable` creation, the initial value's shape ((64, 784)) is not compatible with the explicitly supplied `shape` argument ((64, 64)).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[49], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m inputs \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(\u001b[39m784\u001b[39m,), name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdigits\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m x \u001b[39m=\u001b[39m RBFLayer(\u001b[39m64\u001b[39m, initializer\u001b[39m=\u001b[39mMuInitializer(randomset))(inputs)\n\u001b[0;32m----> 5\u001b[0m x \u001b[39m=\u001b[39m RBFLayer(\u001b[39m64\u001b[39;49m, initializer\u001b[39m=\u001b[39;49mMuInitializer(randomset))(x)\n\u001b[1;32m      6\u001b[0m outputs \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mDense(\u001b[39m10\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m\"\u001b[39m)(x)\n\u001b[1;32m      8\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39minputs, outputs\u001b[39m=\u001b[39moutputs)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "Cell \u001b[0;32mIn[48], line 13\u001b[0m, in \u001b[0;36mRBFLayer.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m, input_shape):\n\u001b[0;32m---> 13\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcenters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_weight(\n\u001b[1;32m     14\u001b[0m         name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcenters\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     15\u001b[0m         shape\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_dim, input_shape[\u001b[39m1\u001b[39;49m]),\n\u001b[1;32m     16\u001b[0m         initializer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minitializer,\n\u001b[1;32m     17\u001b[0m         trainable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_weight(\n\u001b[1;32m     19\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmas\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m         shape\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dim,),\n\u001b[1;32m     21\u001b[0m         initializer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mones\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m         trainable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[0;31mValueError\u001b[0m: In this `tf.Variable` creation, the initial value's shape ((64, 784)) is not compatible with the explicitly supplied `shape` argument ((64, 64))."
          ]
        }
      ],
      "source": [
        "# The Gaussian RBF Layer model\n",
        "\n",
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = RBFLayer(64, initializer=MuInitializer(randomset))(inputs)\n",
        "x = RBFLayer(64, initializer=MuInitializer(randomset))(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.2)\n",
        "\n",
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "\n",
        "print(\"test loss, test acc:\", results)\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "predicted_labels = tf.argmax(predictions, axis=1)\n",
        "accuracy = accuracy_score(y_test, predicted_labels)\n",
        "\n",
        "print(f\"Accuracy score:{accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
